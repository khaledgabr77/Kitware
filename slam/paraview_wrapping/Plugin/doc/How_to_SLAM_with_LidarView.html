<!-- File auto-generated from How_to_SLAM_with_LidarView.md using https://www.browserling.com/tools/markdown-to-html -->
<h1 id="howtoslamwithlidarview">How to SLAM with LidarView ?</h1>

<ul>
<li><a href="#installing-lidarview-or-one-of-its-derivative-with-slam-support">Installing LidarView or one of its derivative with SLAM support</a></li>

<li><a href="#using-slam-in-lidarview">Using SLAM in LidarView</a></li>

<li><a href="#saving-and-exporting-slam-outputs">Saving and exporting SLAM outputs</a>


<ul>
<li><a href="#saving-trajectory">Saving trajectory</a></li>

<li><a href="#saving-keypoint-maps">Saving keypoint maps</a></li>

<li><a href="#saving-aggregated-frames">Saving aggregated frames</a></li>

<li><a href="#aggregate-scans-then-visualize-and-export-them">Aggregate scans then visualize and export them</a></li>

<li><a href="#directly-aggregate-all-points-in-a-las-file">Directly aggregate all points in a LAS file</a></li></ul>
</li>

<li><a href="#slam-parameters-tuning">SLAM parameters tuning</a>


<ul>
<li><a href="#environment-type">Environment type</a></li>

<li><a href="#mobile-platform-carrying-the-lidar-sensor">Mobile platform carrying the LiDAR sensor</a></li>

<li><a href="#increasing-the-processing-speed">Increasing the processing speed</a></li></ul>
</li>
</ul>

<p>This document presents some tips on how to use SLAM algorithm in LidarView, or one of its derived distribution. Even if this SLAM is embedded in a Paraview plugin and is therefore directly usable in Paraview, we will focus on its use in LidarView (as we consider here LiDAR data, LidarView  seems a better choice for most use-cases and display).</p>

<p>Since 2020, this SLAM plugin is natively included and available in <a href="https://www.paraview.org/lidarview/">LidarView</a>.</p>

<h2 id="installinglidarvieworoneofitsderivativewithslamsupport">Installing LidarView or one of its derivative with SLAM support</h2>

<p>Pre-built binaries of LidarView with this SLAM plugin are available for download <a href="https://drive.google.com/drive/folders/1etSkoaR_863MYyRNXgv6PFTiUcggBAcx?usp=sharing">here</a>.</p>

<p>As these binaries may not always be up-to-date with the latest SLAM release, you may want to compile LidarView with SLAM support from source.
Follow <a href="https://gitlab.kitware.com/LidarView/lidarview-core/-/blob/master/Documentation/LidarView_Developer_Guide.md">LidarView's Developer Guide</a> instructions to build LidarView on Windows or Linux.</p>

<p><em><strong>IMPORTANT</strong>: to enable SLAM support, ensure  your CMake configuration has these options set to <code>True</code> :</em></p>

<pre><code>-DENABLE_ceres=True
-DENABLE_nanoflann=True
-DENABLE_pcl=True
-DLIDARVIEW_BUILD_SLAM=True 
</code></pre>

<p><code>LidarSlamPlugin</code> should be automatically loaded at LidarView's startup. If not, ensure <strong>Advanced features</strong> are enabled (in <strong>Help</strong> or  <strong>Tools</strong> > <strong>Debugging</strong>), then select <strong>Advance</strong> > <strong>Tools</strong> > <strong>Manage Plugins</strong> > <strong>Load New</strong>. Browse to your LidarView install directory and select the <code>libLidarSlamPlugin.so</code> / <code>LidarSlamPlugin.dll</code> (this file can normally be found under <code>&lt;lv_build&gt;/install/lib/plugins/</code> on Linux or <code>&lt;lv_build&gt;/install/bin/plugins/</code> on Windows).</p>

<h2 id="usingslaminlidarview">Using SLAM in LidarView</h2>

<p>LidarView's SLAM has been tested on <code>.pcap</code> files aquired from several common LiDAR sensors including:</p>

<ul>
<li>Velodyne (VLP-16, VLP-32c, HDL-32, HDL-64, VLS-128)</li>

<li>Ouster (OS0/1/2-32/64/128)</li>

<li>Hesai (Pandar128)</li>
</ul>

<p>Please note that your default LidarView application may not include all the vendors-specific interpreters to decode all these LiDAR sensors data.</p>

<ol>
<li><p>Open LidarView. Make sure <strong>Advanced Features</strong> are enabled in <strong>Help</strong> tab.</p>

<p><em><strong>Note</strong> : In some LidarView applications, this option is available under <strong>Tools</strong> tab > <strong>Debugging</strong>.</em></p>

<p><img src="enable_advance_feature.png" alt="Enable advance feature" /></p></li>

<li><p>Under <strong>Views</strong> tab, enable <strong>Pipeline Browser</strong> and <strong>Properties</strong>. </p>

<p><img src="enable_views_panels.png" alt="Enable views panels" /></p></li>

<li><p>Open a previously recorded <code>.pcap</code> file (or set up a stream source) associated with its LiDAR calibration file.</p></li>

<li><p>In <strong>Pipeline browser</strong>, select <strong>Frame</strong> (the pointcloud source). Then click on <strong>Filters</strong> tab > <strong>Alphabetical</strong> > <strong>SLAM</strong>. Select a SLAM filter: pick <strong>SLAM (online)</strong> to perform a real-time test with live display, or <strong>SLAM (offline)</strong> for a full process, displaying only final trajectory and maps.</p>

<p><em><strong>Tip</strong> : After having selected <strong>Frame</strong> , you can also hit <code>Ctrl+space</code> and then type <code>slam</code> in filter search bar.</em></p>

<p><img src="create_slam_filter.png" alt="Create SLAM filter" /></p></li>

<li><p>Depending on the SLAM version being used, a new input dialog may appear:</p>

<ul>
<li>Click on the <strong>Point Cloud</strong> input port, select the <strong>Frame</strong> entry. </li>

<li>Click on the <strong>Calibration</strong> input port, select the <strong>Calibration</strong> entry. </li>

<li>Hit <strong>OK</strong> when done.</li></ul>

<p><em><strong>Note</strong>: In some SLAM versions, this calibration is optional, and is not asked by this dialog.</em></p>

<p><img src="select_slam_filter_inputs.png" alt="Select SLAM filter inputs" /></p></li>

<li><p>Under <strong>Properties</strong> panel, modify the parameters if needed (see section <a href="#slam-parameters-tuning">SLAM parameters tuning</a>), then hit <strong>Apply</strong>.</p>

<ul>
<li>If you chose <strong>online SLAM</strong>, a white frame will appear. Hit play button to play back data through the entire recording and watch it SLAM in real time.</li>

<li>If you chose <strong>offline SLAM</strong>, nothing new will show up after you hit <strong>Apply</strong>, but that's normal : the computer is working hard to run SLAM on all frames. When the processing is done, it will display its results.</li></ul></li>

<li><p>You're all done! Now, you can modify the display settings to fit your needs (e.g. color pointclouds using intensity field, modify the points size, hide/show some SLAM outputs, ...).</p></li>
</ol>

<h2 id="savingandexportingslamoutputs">Saving and exporting SLAM outputs</h2>

<p>Once SLAM has completed, it could be useful to save some results for later use.</p>

<h3 id="savingtrajectory">Saving trajectory</h3>

<p>You can export the trajectory (for example as a <code>.csv</code> or <code>.poses</code> file) to avoid running the SLAM again. To save it, select the <strong>Trajectory</strong> output in the <strong>Pipeline Browser</strong> panel, then hit <code>Ctrl+s</code> (or <strong>Advance</strong> tab > <strong>File</strong> > <strong>Save Data</strong>), and choose the output format and name in the dialog window.</p>

<p>Later, to load the trajectory back in LidarView, you can drag and drop the <code>.poses</code> file in LidarView, or <strong>Advance</strong> tab > <strong>File</strong> > <strong>Open</strong>.</p>

<h3 id="savingkeypointmaps">Saving keypoint maps</h3>

<p>Keypoint maps are the local downsampled aggregations of registered keypoints from previous frames. It provides a nice light-weight insight of the reconstructed scene, and helps supervising or interpreting the SLAM behavior and results.</p>

<p>To save SLAM keypoints maps, select the map output you want to save in the <strong>Pipeline Browser</strong> panel, then hit <code>Ctrl+s</code> (or <strong>Advance</strong> tab > <strong>File</strong> > <strong>Save Data</strong>), and choose the output format and name in the dialog window. Common pointclouds formats are <code>csv</code>, <code>pcd</code>, <code>las</code>, <code>ply</code> or <code>vtp</code>.</p>

<h3 id="savingaggregatedframes">Saving aggregated frames</h3>

<p>If the visualization of the maps isn't enough for your application, and you need to aggregate all points from all previous frames, this is possible too, but less straightforward.</p>

<p>There are two ways to export aggregated scans:</p>

<ul>
<li>The first one allows to aggregate previous LiDAR scans into a single pointcloud that can be visualized in LidarView or Paraview. This pointcloud can be optionally downsampled then saved on disk. As this method aggregates points before saving them, the memory consumption is important and can lead to saturation.</li>

<li>The second method uses a previously saved trajectory to aggregate successive scans by directly appending data to a LAS file on disk.</li>
</ul>

<h4 id="aggregatescansthenvisualizeandexportthem">Aggregate scans then visualize and export them</h4>

<p><em><strong>Note</strong>: Be careful, the aggregation of ALL previous points may represent a huge pointcloud! With ~1 000 000 points per second and the associated measurements (intensity, time, laser ID, etc.), each minute of recording represents several GB of data! Depending on your machine specifications, this may not fit into memory and lead to an annoying crash of the application.</em></p>

<p>To visualize all frames as a single aggregated pointcloud, you need to instanciate several filters to aggregate all scans using the computed trajectory (sensor path estimated by SLAM):</p>

<ol>
<li><p>Select the <strong>Trailing frame</strong> entry and set the desired number of trailing frames (0 meaning only the last frame, and, for example, 10 displaying the current frame and the 10 previous ones). Be careful, a big number of trailing frames may lead to important processing duration and memory consumption. Click on <strong>Apply</strong>. You should now see all the frames aggregated in a non-sense way (all points being displayed using their coordinates relative to the sensor at the time of acquisition).</p></li>

<li><p>Instantiate a <strong>Temporal Transform Applier</strong> filter using the <strong>Trailing Frame</strong> as point cloud entry, and the output SLAM <strong>Trajectory</strong> for trajectory entry. Depending on the number of trailing frames, the transformation and aggregation of pointclouds may be long. When it succeeds, you should now see all points being correctly registered. If the colors look strange, check that you are displaying the <code>intensity</code> array in the main toolbar.</p></li>
</ol>

<p><img src="aggregated_frames.png" alt="Aggregated frames" /></p>

<p>These first steps allow you to visualize all the aggregated points in LidarView. If you want to optionally downsample this point cloud and save it on disk, please follow these additional steps:</p>

<ol>
<li><p>Instantiate a <strong>Merge Blocks</strong> filter on the output of the <strong>Temporal Transform Applier</strong>.</p>

<p>If you want to subsample the aggregated pointcloud by merging close points, make sure that the advanced properties of the filter are enabled by toggling the little gear wheel in the <strong>Properties</strong> panel. You can then specify the <em>Tolerance</em> and <em>Tolerance Is Absolute</em> parameters. For example, to aggregate all points but keeping only a single point per 5 cm cube, set <em>Tolerance</em> to 0.05 and enable <em>Tolerance Is Absolute</em>.</p>

<p>Click on <strong>Apply</strong>. The process can take some seconds. When it has finished, all points are now merged into a single (optionally downsampled) pointcloud.</p></li>

<li><p>Instantiate an <strong>Extract Surface</strong> filter on the ouput of the <strong>Merge Blocks</strong>. This will convert the underlying structure of the pointcloud, allowing more available filters or file types for storage.</p></li>

<li><p>As usual, save aggregated frames by selecting the desired output <strong>Extract Surface</strong>, hit <code>Ctrl+s</code>, and choose the output format (CSV, PLY, VTP) and name.</p></li>
</ol>

<h4 id="directlyaggregateallpointsinalasfile">Directly aggregate all points in a LAS file</h4>

<p>This method directly appends points to a LAS file on disk, which has the advantage to avoid saturating the RAM.</p>

<ol>
<li><p><a href="#saving-trajectory">Save the SLAM trajectory</a> on disk as a <code>.poses</code> CSV file.</p></li>

<li><p>Open a new LidarView session, then load your pcap recording.
Load the previously exported SLAM trajectory using <strong>Advance</strong> > <strong>File</strong> > <strong>Open</strong>.</p></li>

<li><p>Instantiate a <strong>Temporal Transform Applier</strong> filter using the <strong>Frame</strong> as point cloud entry, and the output SLAM <strong>Trajectory</strong> for trajectory entry.</p></li>

<li><p>Save the output of the <strong>Temporal Transform Applier</strong> by hitting <code>Ctrl+s</code>, selecting the <strong>LAS point cloud file</strong> format, and specifying the output file name before validating. A new dialog will appear to configure the LAS file writer, where you can modify the parameters to your needs. For example, if you want to aggregate points from frame 100 to 500, but using only points from 1 frame out of 3, specify <em>First Frame = 100</em>, <em>Last Frame = 500</em>, <em>Frame Stride = 3</em>.
The export can be quite long (from a few seconds to several minutes) as each specified frame needs to be processed.</p></li>
</ol>

<h2 id="slamparameterstuning">SLAM parameters tuning</h2>

<p>The default SLAM parameters should be a good compromise to run the SLAM for most outdoor urban environments with the LiDAR sensor mounted on top of a vehicle.
However, the parameters should be adapted to your specific needs or environment to have an optimal result.</p>

<p><em><strong>Note</strong>: To see all parameters, make sure that the advanced properties of the SLAM filter are enabled by toggling the little gear wheel in the <strong>Properties</strong> panel.</em></p>

<p>Here are some hints to help you tune some of the main parameters. These are some typical parameters preset to consider depending on your dataset characteristics.</p>

<h3 id="environmenttype">Environment type</h3>

<p>The type of environment influences a lot the number and quality of extracted keypoints. A feature-poor scene will need more and denser keypoints to give nice results. However, we want to keep this number of keypoints as small as possible to reduce memory consumption and problem dimensionality.</p>

<ul>
<li><strong><em>Outdoor scene</em></strong>


<ul>
<li><strong>Keyframe distance/angle threshold</strong>: 0.5-1 m distance, 2-5° angle.</li>

<li><strong>Edges/Planes map resolution</strong>: 30 cm for edges, 60 cm for planes.</li></ul>
</li>

<li><strong><em>Indoor scene</em></strong>


<ul>
<li><strong>Keyframe distance/angle threshold</strong>: 0.1-0.5 m distance, 5° angle.</li>

<li><strong>Minimum distance to sensor</strong>: 0.5 m</li>

<li><strong>Edges/Planes map resolution</strong>: 20 cm for edges, 30 cm for planes</li>

<li><strong>Rolling grid resolution</strong>: 3 m.</li></ul>
</li>

<li><strong><em>Poor geometric scene or scene with some strong invariance</em></strong>: corridor, fields, highway, forest...


<ul>
<li><strong>Keyframe distance/angle threshold</strong>: 0 m distance, 0° angle (disabled).</li>

<li><strong>Use Blobs</strong>: enabled</li>

<li><strong>ICP-Optimization iterations</strong>: 4</li>

<li><strong>Edges/Planes map resolution</strong>: 20 cm for edges, 30 cm for planes</li></ul>
</li>
</ul>

<h3 id="mobileplatformcarryingthelidarsensor">Mobile platform carrying the LiDAR sensor</h3>

<p>The type of the mobile platform has a great impact on the motion model, or in other words, how we estimate and compensate the motion since the previous scan. Smooth motions will be much easier and lead to more stable results (as it provides continuous and approximately constant speed motion hypothesis to interpolate new pose), whereas high-frequency motions or fast moving platforms will be less robust.</p>

<ul>
<li><strong><em>Vehicle</em></strong>: LiDAR sensor mounted on top of a car


<ul>
<li><strong>Ego-Motion mode</strong>: Motion extrapolation</li>

<li><strong>Undistortion mode</strong>: Once / Refined</li></ul>
</li>

<li><strong><em>Drone</em></strong>: LiDAR sensor carried by a drone


<ul>
<li><strong>Ego-Motion mode</strong>: Motion extrapolation / Registration on previous frame</li>

<li><strong>Undistortion mode</strong>: Refined</li></ul>
</li>

<li><strong><em>Pedestrian</em></strong>: man-held LiDAR sensor


<ul>
<li><strong>Ego-Motion mode</strong>: Motion extrapolation / Registration on previous frame / Disabled</li>

<li><strong>Undistortion mode</strong>: Disabled / Refined</li></ul>
</li>
</ul>

<h3 id="increasingtheprocessingspeed">Increasing the processing speed</h3>

<p>To increase the processing speed, consider also tweaking these parameters:</p>

<ul>
<li><p><strong>Number of threads</strong>: Maximum number of threads used for parallel processing. Allowing several threads (about 4) increases SLAM processing speed, skipping less frames, and thus improving result.</p></li>

<li><p><strong>Keypoints maps update step</strong>: If you don't need the map display to be refreshed at each new frame, you should consider increasing this value. Setting it to 10 will only update maps every 10th frame (1 second at 600 rpm), which is far enough for a nice visualization. This will save some output conversion time.</p></li>

<li><p><strong>LidarView play speed</strong>: This is not specific to the SLAM filter, but LidarView is controlling the playback speed of the LiDAR recording. It can be set from the VCR toolbar. For example, <em>Speed = x1</em> will play at real speed, <em>Speed = x3</em> will play 3 times faster. If the SLAM algorithm isn't fast enough to process all incoming frames, it will drop some of them. If your LiDAR is slowly moving or with smooth motion, this not a problem. However, if it skips too many frames compared to the LiDAR motion, consider choosing the <em>Speed = All Frames</em>, which will play as fast as possible but ensuring that all frames are processed.</p></li>
</ul>
